from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

""" æ¨¡å‹ä»‹ç»
MiniCPM 3.0 æ˜¯ä¸€ä¸ª 4B å‚æ•°é‡çš„è¯­è¨€æ¨¡å‹ï¼Œç›¸æ¯” MiniCPM1.0/2.0ï¼ŒåŠŸèƒ½æ›´åŠ å…¨é¢ï¼Œç»¼åˆèƒ½åŠ›å¤§å¹…æå‡ï¼Œå¤šæ•°è¯„æµ‹é›†ä¸Šçš„æ•ˆæœæ¯”è‚©ç”šè‡³è¶…è¶Šä¼—å¤š 7B-9B æ¨¡å‹ã€‚

æ”¯æŒå·¥å…·è°ƒç”¨ğŸ› ï¸ï¼ˆFunction Callingï¼‰å’Œä»£ç è§£é‡Šå™¨ğŸ’»ï¼ˆCode Interpreterï¼‰ï¼šBerkeley Function Calling Leaderboard (BFCL) ä¸Šå–å¾— 9B è§„æ¨¡ä»¥ä¸‹ SOTAï¼Œè¶…è¶Š GLM-4-9B-Chatã€Qwen2-7B-Instructã€‚
è¶…å¼ºçš„æ¨ç†èƒ½åŠ›ğŸ§®ï¼šæ•°å­¦èƒ½åŠ›æ–¹é¢ï¼ŒMathBench ä¸Šçš„æ•ˆæœè¶…è¶Š GPT-3.5-Turbo ä»¥åŠå¤šä¸ª 7B-9B æ¨¡å‹ã€‚åœ¨éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ LiveCodeBench ä¸Šï¼Œæ•ˆæœè¶…è¶Š Llama3.1-8B-Instructã€‚
å‡ºè‰²çš„ä¸­è‹±æ–‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›ğŸ¤–ï¼šè‹±æ–‡æŒ‡ä»¤éµå¾ª IFEvalã€ä¸­æ–‡æŒ‡ä»¤éµå¾ª FollowBench-zh æ•ˆæœè¶…è¶Š GLM-4-9B-Chatã€Qwen2-7B-Instructã€‚
é•¿æ–‡æœ¬èƒ½åŠ›ï¼šåŸç”Ÿæ”¯æŒ 32k ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œ32k é•¿åº¦å†…å¤§æµ·æé’ˆå…¨ç»¿ã€‚æå‡º LLMxMapReduce ï¼Œç†è®ºå¯å¤„ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦è¾¾åˆ° +âˆï¼Œåœ¨ç»¼åˆæ€§é•¿æ–‡æœ¬è¯„æµ‹åŸºå‡† InfiniteBench å¹³å‡å¾—åˆ†è¶…è¶ŠGPT-4ã€KimiChatç­‰æ ‡æ†æ¨¡å‹ã€‚
RAGèƒ½åŠ›ï¼šæˆ‘ä»¬å‘å¸ƒäº† MiniCPM RAG å¥—ä»¶ã€‚åŸºäº MiniCPM ç³»åˆ—æ¨¡å‹çš„ MiniCPM-Embeddingã€MiniCPM-Reranker åœ¨ä¸­æ–‡ã€ä¸­è‹±è·¨è¯­è¨€æ£€ç´¢æµ‹è¯•ä¸­å–å¾— SOTA è¡¨ç°ï¼›é’ˆå¯¹ RAG åœºæ™¯çš„ MiniCPM3-RAG-LoRA åœ¨å¼€æ”¾åŸŸé—®ç­”ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šè¶…è¶Š Llama3-8Bã€Baichuan2-13B ç­‰æ¨¡å‹ã€‚
"""


class ResponseWrapper:
    """åŒ…è£…APIè¿”å›çš„å“åº”å†…å®¹ï¼Œæ”¯æŒ .content è®¿é—®ã€‚"""

    def __init__(self, content):
        self.content = content


class MiniCPMClient:
    """GPU:æµç•…è¿è¡Œè¦æ±‚8G+  8Gæ¨ç†æ¯”è¾ƒä¹…,å¤§æ¦‚40-60æ¥ç§’,å…·ä½“çœ‹å‹å·"""
    def __init__(self,
                 model_path="OpenBMB/MiniCPM3-4B",
                 device="cuda",
                 cache_dir="../model"):
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            cache_dir=cache_dir,
            local_files_only=True
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map=self.device,
            trust_remote_code=True,
            cache_dir=cache_dir,
            local_files_only=True
        )

    def invoke(self, messages, max_tokens=1024, top_p=0.7, temperature=0.7):
        model_inputs = self.tokenizer.apply_chat_template(
            messages,
            return_tensors="pt",
            add_generation_prompt=True
        ).to(self.device)

        model_outputs = self.model.generate(
            model_inputs,
            max_new_tokens=max_tokens,
            top_p=top_p,
            temperature=temperature
        )

        output_token_ids = [
            model_outputs[i][len(model_inputs[i]):] for i in range(len(model_inputs))
        ]

        content = self.tokenizer.batch_decode(
            output_token_ids, skip_special_tokens=True
        )[0]

        return ResponseWrapper(content)


# æµ‹è¯•ç¤ºä¾‹
if __name__ == "__main__":
    client = MiniCPMClient()
    prompt = "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹"
    message = "æ¨è5ä¸ªåŒ—äº¬çš„æ™¯ç‚¹"
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": message}
    ]

    response = client.invoke(messages)

    print(response.content)
